{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "assert platform.python_version_tuple() >= (\"3\", \"6\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "A straight line function where activation is proportional to input ( which is the weighted sum from neuron ).\n",
    "\n",
    "### Function\n",
    "\n",
    "$$ \n",
    "R(z,m) = \\begin{Bmatrix} z*m \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z,m):\n",
    "\treturn m*z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "R'(z,m) = \\begin{Bmatrix} m \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prime(z,m):\n",
    "\treturn m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- It gives a range of activations, so it is not binary activation.\n",
    "- We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that.\n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- For this function, derivative is a constant. That means, the gradient has no relationship with X.\n",
    "- It is a constant gradient and the descent is going to be on constant gradient.\n",
    "- If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x) !\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU\n",
    "**Exponential Linear Unit** or its widely known name **ELU** is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.\n",
    "\n",
    "ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "\n",
    "### Function\n",
    "\n",
    "$$\n",
    "R(z) = \\begin{Bmatrix} z & z > 0 \\\\\n",
    "α.( e^z – 1) & z <= 0 \\end{Bmatrix}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z,alpha):\n",
    "\treturn z if z >= 0 else alpha*(np.exp(z) -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "R'(z) = \\begin{Bmatrix} 1 & z>0 \\\\\n",
    "α.e^z & z<0 \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_prime(z,alpha):\n",
    "\treturn 1 if z > 0 else alpha*np.exp(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- \n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "A recent invention which stands for Rectified Linear Units. The formula is deceptively simple: $max(0,z)$. \n",
    "Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid (i.e. the ability to learn nonlinear functions), but with better performance.\n",
    "\n",
    "### Function\n",
    "\n",
    "$$\n",
    "R(z) = \\begin{Bmatrix} z & z > 0 \\\\\n",
    "0 & z <= 0 \\end{Bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "  return max(0, z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "R'(z) = \\begin{Bmatrix} 1 & z>0 \\\\\n",
    "0 & z<0 \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_prime(z):\n",
    "  return 1 if z > 0 else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- \n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeakyReLU\n",
    "LeakyRelu is a variant of ReLU. Instead of being 0 when :math:`z < 0`, a leaky ReLU allows a small, non-zero, constant gradient :math:`\\alpha` (Normally, :math:`\\alpha = 0.01`). However, the consistency of the benefit across tasks is presently unclear. [1]_\n",
    "### Function\n",
    "\n",
    "$$\n",
    "R(z) = \\begin{Bmatrix} z & z > 0 \\\\\n",
    "\\alpha z & z <= 0 \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "R'(z) = \\begin{Bmatrix} 1 & z>0 \\\\\n",
    "\\alpha & z<0 \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- \n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "Sigmoid takes a real value as input and outputs another value between 0 and 1. It’s easy to work with and has all the nice properties of activation functions: it’s non-linear, continuously differentiable, monotonic, and has a fixed output range.\n",
    "### Function\n",
    "\n",
    "$$\n",
    "S(z) = \\frac{1} {1 + e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "S'(z) = S(z) \\cdot (1 - S(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- \n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "Softmax function calculates the probabilities distribution of the event over ‘n’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.\n",
    "### Function\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- \n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "### Function\n",
    "\n",
    "$$\n",
    "tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = 2\\sigma(2z) - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "tanh'(z) = 1 - tanh(z)^{2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- \n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
