{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "assert platform.python_version_tuple() >= (\"3\", \"6\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "A straight line function where activation is proportional to input ( which is the weighted sum from neuron ).\n",
    "\n",
    "### Function\n",
    "\n",
    "$$ \n",
    "R(z,m) = \\begin{Bmatrix} z*m \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z,m):\n",
    "\treturn m*z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "R'(z,m) = \\begin{Bmatrix} m \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prime(z,m):\n",
    "\treturn m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- It gives a range of activations, so it is not binary activation.\n",
    "- We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that.\n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- For this function, derivative is a constant. That means, the gradient has no relationship with X.\n",
    "- It is a constant gradient and the descent is going to be on constant gradient.\n",
    "- If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x) !\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU\n",
    "**Exponential Linear Unit** or its widely known name **ELU** is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.\n",
    "\n",
    "ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "\n",
    "### Function\n",
    "\n",
    "$$\n",
    "R(z) = \\begin{Bmatrix} z & z > 0 \\\\\n",
    "α.( e^z – 1) & z <= 0 \\end{Bmatrix}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z,alpha):\n",
    "\treturn z if z >= 0 else alpha*(np.exp(z) -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "R'(z) = \\begin{Bmatrix} 1 & z>0 \\\\\n",
    "α.e^z & z<0 \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_prime(z,alpha):\n",
    "\treturn 1 if z > 0 else alpha*np.exp(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- \n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "A recent invention which stands for Rectified Linear Units. The formula is deceptively simple: $max(0,z)$. Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid (i.e. the ability to learn nonlinear functions), but with better performance.\n",
    "\n",
    "### Function\n",
    "\n",
    "$$\n",
    "R(z) = \\begin{Bmatrix} z & z > 0 \\\\\n",
    "0 & z <= 0 \\end{Bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "  return max(0, z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "$$\n",
    "R'(z) = \\begin{Bmatrix} 1 & z>0 \\\\\n",
    "0 & z<0 \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_prime(z):\n",
    "  return 1 if z > 0 else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- \n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "\n",
    "## LeakyReLU\n",
    "\n",
    "\n",
    "## Softmax\n",
    "\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "\n",
    "## Tanh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
