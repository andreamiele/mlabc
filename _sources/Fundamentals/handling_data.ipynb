{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 0.1: Handling Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup\n",
    "To get started with machine learning in Python, we need to set up a development environment that includes the necessary libraries and tools. In this section, we'll go through the steps to set up a virtual environment and install Python, NumPy, Pandas, Matplotlib, and Jupyter Notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python\n",
    "Python is the programming language we'll be using for machine learning. To install Python, we recommend using the Anaconda distribution, which includes all the required libraries and tools.\n",
    "You can download Anaconda from the official website: https://www.anaconda.com/products/individual\n",
    "\n",
    "Once you've downloaded the installer, run it and follow the prompts to install Anaconda."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Set Up a Virtual Environment\n",
    "A virtual environment is a self-contained environment that allows us to install and manage packages without affecting the global Python installation. To set up a virtual environment, open the Anaconda Prompt (Windows) or Terminal (Mac/Linux) and enter the following command:\n",
    "\n",
    "`conda create --name myenv`\n",
    "\n",
    "This will create a new environment named \"myenv\". You can replace \"myenv\" with any name you like.\n",
    "\n",
    "To activate the virtual environment, enter the following command:\n",
    "\n",
    "`conda activate myenv`\n",
    "\n",
    "Replace \"myenv\" with the name of your virtual environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries\n",
    "To install the required libraries (NumPy, Pandas, Matplotlib, Jupyter Notebook), enter the following command:\n",
    "\n",
    "`conda install numpy pandas matplotlib jupyter`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Jupyter Notebook\n",
    "To launch Jupyter Notebook, enter the following command:\n",
    "\n",
    "`jupyter notebook`\n",
    "\n",
    "This will open a web browser and display the Jupyter Notebook interface. You can create a new notebook and start coding!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start working with data, we need to have a clear understanding of the problem we're trying to solve and the requirements for the data and model. In this section, we'll cover the key aspects of problem formulation, including defining the problem, choosing appropriate metrics for evaluation, and deciding on the input and output formats for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the problem\n",
    "The first step in problem formulation is to define the problem we're trying to solve. This includes identifying the type of problem (classification, regression, clustering, etc.), the scope of the problem (number of classes, size of dataset, etc.), and any specific constraints or requirements.\n",
    "\n",
    "For example, if we're working on a classification problem to identify handwritten digits, we need to decide on the number of classes (10 digits) and the size of the dataset (e.g., 60,000 training images and 10,000 test images). We also need to consider any specific constraints, such as the need for real-time classification or the availability of computing resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose appropriate metrics for evaluation\n",
    "Once we've defined the problem, we need to choose appropriate metrics for evaluating the performance of our model. The choice of metrics depends on the type of problem and the goals of the project.\n",
    "\n",
    "For classification problems, common evaluation metrics include accuracy, precision, recall, F1-score, and area under the ROC curve. For regression problems, common metrics include mean squared error, mean absolute error, and R-squared.\n",
    "\n",
    "It's important to choose metrics that align with the goals of the project. For example, if the goal is to minimize false positives in a medical diagnosis system, we should focus on metrics that measure precision rather than recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on Input and Output formats\n",
    "Finally, we need to decide on the input and output formats for our model. This includes deciding on the features to use as inputs, as well as the format of the output (e.g., a single scalar value for regression, a probability distribution over classes for classification).\n",
    "\n",
    "In some cases, we may need to preprocess the data to extract relevant features or transform the data into a suitable format. For example, in image classification, we might use pixel values as input features and one-hot encoding to represent the output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with machine learning models, we often use a design matrix to represent the input data and a target vector to represent the output labels.\n",
    "\n",
    "The design matrix is a matrix where each row represents an input sample and each column represents a feature or attribute. The target vector is a vector where each element represents the corresponding label or target for the corresponding input sample.\n",
    "\n",
    "We can represent the design matrix as X, where X has dimensions (n_samples, n_features), and the target vector as y, where y has dimensions (n_samples, 1).\n",
    "\n",
    "The input features are denoted as $x_{i,j}$, where i is the index of the sample and j is the index of the feature. The corresponding target or label for the ith sample is denoted as $y_i$.\n",
    "\n",
    "Here are the equations for the inputs, design matrix and target vector:\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} $$ \n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix} x_{11} & x_{12} & \\dots & x_{1n} \\\\ x_{21} & x_{22} & \\dots & x_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{m1} & x_{m2} & \\dots & x_{mn} \\end{bmatrix} $$ \n",
    "\n",
    "$$\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we can represent the design matrix and target vector as NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the design matrix X and target vector y\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([0, 1, 0])\n",
    "\n",
    "# Print the dimensions of X and y\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X shape: (3, 3)\n",
    "y shape: (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, X has dimensions (3, 3), indicating that there are 3 samples and 3 features, and y has dimensions (3,), indicating that there are 3 targets.\n",
    "\n",
    "In summary, the design matrix and target vector are essential components of machine learning models and are used to represent the input and output data, respectively. The design matrix is a matrix where each row represents an input sample and each column represents a feature, and the target vector is a vector where each element represents the corresponding label or target for the corresponding input sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Tensors and Common Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are multi-dimensional arrays commonly used in machine learning. \n",
    "\n",
    "The number of dimensions of a tensor is known as its **rank**, and each dimension is often referred to as an **axis**. The **shape** of a tensor describes the number of values along each axis, and the number of entries along a specific axis is also referred to as **dimension**. For instance, it's important to note that a 3-dimensional vector is not the same as a 3-dimensional tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Tensors\n",
    "A tensor is a generalization of a vector or matrix to higher dimensions. In NumPy, we can represent tensors using the ndarray class. The number of dimensions of a tensor is called its rank, and the size of each dimension is called its shape.\n",
    "\n",
    "Here are some examples of tensors:\n",
    "- Scalar: A tensor of rank 0, representing a single value.\n",
    "- Vector: A tensor of rank 1, representing a sequence of values.\n",
    "- Matrix: A tensor of rank 2, representing a table of values.\n",
    "- Higher-dimensional tensor: A tensor of rank > 2, representing a multidimensional array of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Tensor Operations\n",
    "We can perform various operations on tensors, including addition, subtraction, multiplication, division, and element-wise operations. In NumPy, these operations are performed using the same syntax as for scalar operations.\n",
    "\n",
    "In Python, [NumPy]() provides support for tensors in the form of ndarray objects, which can be manipulated using a comprehensive set of operations, including creating, sorting, selecting, linear algebra, and statistical operations. [PyTorch]() offers a NumPy-like API for manipulating tensors, which can also be located on a GPU for faster computations. [TensorFlow]() also supports tensors, but its API is generally considered more cumbersome.\n",
    "\n",
    "Here's some example code for basic tensor operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a scalar tensor\n",
    "x = np.array(5)\n",
    "\n",
    "# Create a vector tensor\n",
    "y = np.array([1, 2, 3])\n",
    "\n",
    "# Create a matrix tensor\n",
    "z = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Addition\n",
    "print(x + 2)  # Output: 7\n",
    "print(y + 1)  # Output: [2 3 4]\n",
    "print(z + np.array([[1, 1], [1, 1]]))  # Output: [[2 3] [4 5]]\n",
    "\n",
    "# Subtraction\n",
    "print(x - 2)  # Output: 3\n",
    "print(y - 1)  # Output: [0 1 2]\n",
    "print(z - np.array([[1, 1], [1, 1]]))  # Output: [[0 1] [2 3]]\n",
    "\n",
    "# Multiplication\n",
    "print(x * 2)  # Output: 10\n",
    "print(y * 2)  # Output: [2 4 6]\n",
    "print(z * np.array([[1, 1], [1, 1]]))  # Output: [[1 2] [3 4]]\n",
    "\n",
    "# Division\n",
    "print(x / 2)  # Output: 2.5\n",
    "print(y / 2)  # Output: [0.5 1.  1.5]\n",
    "print(z / np.array([[2, 2], [2, 2]]))  # Output: [[0.5 1. ] [1.5 2. ]]\n",
    "\n",
    "# Element-wise operations\n",
    "print(np.sin(z))  # Output: [[0.84147098 0.90929743] [0.14112001 -0.7568025 ]]\n",
    "print(np.exp(z))  # Output: [[ 2.71828183  7.3890561 ] [20.08553692 54.59815003]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting and Reshaping Tensors\n",
    "Broadcasting and reshaping are common operations used to manipulate tensors. Broadcasting allows us to perform operations on tensors with different shapes, while reshaping allows us to change the shape of a tensor without changing its data.\n",
    "\n",
    "Here's some example code for broadcasting and reshaping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([[1], [2], [3]])\n",
    "print(x + y)  # Output: [[2 3 4] [3 4 5] [4 5 6]]\n",
    "\n",
    "# Reshaping\n",
    "x = np.array([1, 2, 3, 4, 5, 6])\n",
    "y = x.reshape((2, 3))\n",
    "print(y)  # Output: [[1 2 3] [4 5 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, tensors are the basic building blocks of most machine learning models, and we can perform various operations on tensors, including addition, subtraction, multiplication, division, and element-wise operations. Broadcasting and reshaping are common operations used to manipulate tensors, allowing us to perform operations on tensors with different shapes and change the shape of a tensor without changing its data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Collection and Preprocessing\n",
    "Data collection and preprocessing are critical steps in machine learning. In this section, we'll cover the basics of data collection and preprocessing, including data sources, data cleaning, and feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "There are various sources of data for machine learning, including public datasets, private datasets, and synthetic datasets. Public datasets, such as those provided by Kaggle and UCI Machine Learning Repository, are freely available and commonly used for benchmarking and research. Private datasets, such as those used in industry, may require permissions or licenses to access. Synthetic datasets are generated using models or simulations and can be useful for generating large amounts of data for training models.\n",
    "\n",
    "When selecting a dataset, it's important to consider factors such as data quality, data size, and data relevance to the problem at hand.\n",
    "\n",
    "### Data Cleaning\n",
    "Data cleaning is the process of identifying and correcting errors or inconsistencies in the data. Common data cleaning tasks include removing missing or invalid data, correcting typos or formatting issues, and handling outliers or extreme values.\n",
    "\n",
    "Here's an example code for removing missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Check for missing data\n",
    "print(data.isnull().sum())  # Output: ['column1': 0, 'column2': 3, 'column3': 0]\n",
    "\n",
    "# Remove rows with missing data\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Feature scaling is the process of scaling the values of features to a similar range to improve the performance of machine learning models. Common methods of feature scaling include normalization and standardization.\n",
    "\n",
    "Here's some example code for feature scaling using normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Scale the features using normalization\n",
    "scaler = MinMaxScaler()\n",
    "data[['feature1', 'feature2']] = scaler.fit_transform(data[['feature1', 'feature2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Data augmentation is a technique used in machine learning to increase the size and diversity of a dataset by creating new data from existing data. This can help to improve the performance of machine learning models by reducing overfitting and increasing the robustness of the model to variations in the input data.\n",
    "\n",
    "#### Introduction to Data Augmentation\n",
    "Data augmentation involves applying a set of transformations to the existing data to create new data points. Common transformations include rotation, scaling, cropping, flipping, and adding noise.\n",
    "\n",
    "Here's an example code for data augmentation using the imgaug library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "from skimage import io\n",
    "\n",
    "# Load an image\n",
    "image = io.imread('image.jpg')\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(p=0.5),\n",
    "    iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "    iaa.Affine(rotate=(-45, 45)),\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 0.1*255)),\n",
    "])\n",
    "\n",
    "# Apply the augmentation pipeline to the image\n",
    "image_aug = seq(image=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of Data Augmentation\n",
    "Data augmentation is commonly used in computer vision and natural language processing tasks. In computer vision, data augmentation can help to increase the size and diversity of the training data, allowing the model to learn more robust features and improve its performance on unseen data. In natural language processing, data augmentation can involve techniques such as synonym replacement, paraphrasing, and back-translation to generate new sentences from existing sentences.\n",
    "\n",
    "Here's an example code for data augmentation using the NLPAug library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Load a sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Apply the augmentation pipeline to the sentence\n",
    "sentence_aug = aug.augment(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Feature Extraction and Transformation\n",
    "\n",
    "Feature extraction and transformation are important steps in machine learning that involve selecting and transforming input features to improve the performance of machine learning models. In this section, we'll cover the basics of feature extraction and transformation, including feature selection, feature engineering, and dimensionality reduction.\n",
    "\n",
    "### Feature Selection\n",
    "Feature selection is the process of selecting a subset of relevant features from a larger set of input features. This can help to reduce the complexity of the model and improve its performance by removing irrelevant or redundant features.\n",
    "\n",
    "There are various methods for feature selection, including univariate feature selection, recursive feature elimination, and feature importance ranking.\n",
    "\n",
    "Here's an example code for feature selection using univariate feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Select the top 2 most important features using chi-squared test\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Feature engineering is the process of creating new features from the existing input features to improve the performance of machine learning models. This can involve combining, transforming, or encoding the input features.\n",
    "\n",
    "Common methods of feature engineering include polynomial features, interaction features, and categorical encoding.\n",
    "\n",
    "Here's an example code for feature engineering using polynomial features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "data_poly = poly.fit_transform(data[['feature1', 'feature2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "Dimensionality reduction is the process of reducing the number of input features while preserving as much of the original information as possible. This can help to reduce the complexity of the model and improve its performance by removing noise and redundancy in the input features.\n",
    "\n",
    "Common methods of dimensionality reduction include principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE).\n",
    "\n",
    "Here's an example code for dimensionality reduction using PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data[['feature1', 'feature2']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Handling Missing and Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the challenges in machine learning is dealing with missing and noisy data. Missing data can occur when some of the input features have not been recorded or are not available, while noisy data can occur when there are errors or inconsistencies in the input data.\n",
    "\n",
    "### Handling Missing Data\n",
    "There are several strategies for handling missing data, including:\n",
    "\n",
    "- Deleting the rows or columns with missing data.\n",
    "- Imputing the missing data using various techniques, such as mean imputation, median imputation, and regression imputation.\n",
    "- Using algorithms that are robust to missing data, such as decision trees and random forests.\n",
    "\n",
    "Here's an example code for mean imputation using the pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a dataset with missing data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Perform mean imputation on the missing values\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Noisy Data\n",
    "There are several strategies for handling noisy data, including:\n",
    "\n",
    "**Removing outliers** using various techniques, such as z-score method and interquartile range (IQR) method.\n",
    "Smoothing the data using techniques such as moving average and Gaussian smoothing.\n",
    "Using algorithms that are robust to noisy data, such as decision trees and random forests.\n",
    "Here's an example code for outlier removal using the z-score method using the numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load a dataset with noisy data\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])\n",
    "# Compute the z-scores of the data\n",
    "z_scores = (data - np.mean(data)) / np.std(data)\n",
    "# Identify outliers based on a threshold\n",
    "outliers = np.where(np.abs(z_scores) > 3)\n",
    "\n",
    "# Remove the outliers\n",
    "data_clean = np.delete(data, outliers)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
