{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 0.2: Assessing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO COME: \n",
    "- Evaluation metrics (accuracy, precision, recall, F1-score)\n",
    "- Cross-validation and model selection\n",
    "- Bias and fairness in evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup\n",
    "\n",
    "Before we can evaluate the performance of a machine learning model, we need to set up our environment with the necessary libraries and tools. Here are some commonly used Python libraries for machine learning:\n",
    "\n",
    "- Scikit-learn: A popular library for machine learning in Python, with support for a variety of models and evaluation metrics.\n",
    "- TensorFlow: A powerful library for deep learning, with support for building and training complex neural networks.\n",
    "- PyTorch: Another popular library for deep learning, with a user-friendly interface and support for dynamic computation graphs.\n",
    "Here's an example code for installing Scikit-learn and importing it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Evaluation Metrics\n",
    "\n",
    "In machine learning, evaluation metrics are used to assess the performance of a model on a given dataset. Evaluation metrics can be broadly classified into two categories: classification metrics and regression metrics.\n",
    "\n",
    "### 2.1 Classification Metrics\n",
    "\n",
    "Classification is a supervised learning task in which the goal is to assign input data points to one of several discrete categories. Examples include spam detection, image classification, and sentiment analysis.\n",
    "\n",
    "The most commonly used classification metrics are accuracy, precision, recall, and F1-score. Let's define these metrics mathematically:\n",
    "\n",
    "**Accuracy** measures the proportion of correct predictions among all predictions. It is defined as:\n",
    "\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$ \n",
    "\n",
    "where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives.\n",
    "\n",
    "**Precision** measures the proportion of true positives among all positive predictions. It is defined as:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "**Recall** measures the proportion of true positives among all actual positives. It is defined as:\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "**F1-score** is a weighted average of precision and recall, which balances the trade-off between them. It is defined as:\n",
    "\n",
    "$$ F1\\text{-}score = 2\\times\\frac{Precision\\times Recall}{Precision+Recall} $$\n",
    "\n",
    "We can use Python libraries such as Scikit-learn to compute these evaluation metrics for our classification models. Here's an example code for computing accuracy, precision, recall, and F1-score for a binary classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "y_true = [0, 1, 1, 0, 1, 0]\n",
    "y_pred = [0, 1, 0, 0, 1, 1]\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to these metrics, we may also need to compute other classification metrics such as area under the curve, confusion matrix, or ROC curve. We can use Scikit-learn to compute these metrics as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Regression Metrics\n",
    "\n",
    "Regression is a supervised learning task in which the goal is to predict a continuous output variable given one or more input variables. Examples include house price prediction, stock price prediction, and weather forecasting.\n",
    "\n",
    "The most commonly used regression metrics are mean squared error, mean absolute error, and R-squared. Let's define these metrics mathematically:\n",
    "\n",
    "- Mean squared error (MSE) measures the average squared difference between the predicted and actual values. It is defined as:\n",
    "- \n",
    "$$ MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where $y_i$ is the actual value, $\\hat{y}_i$ is the predicted value, and $n$ is the number of data points.\n",
    "\n",
    "- Mean absolute error (MAE) measures the average absolute difference between the predicted and actual values. It is defined as:\n",
    "- \n",
    "$$ MAE= \\frac{1}{n}\\sum_{i=1}^n|y_i - \\hat{y}_i| $$\n",
    "\n",
    "- R-squared (RÂ²) measures the proportion of variance in the output variable that can be explained by the input variables. It is defined as:\n",
    "- \n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2} $$\n",
    "where $\\bar{y}$ is the mean of the output variable.\n",
    "\n",
    "We can use Python libraries such as Scikit-learn to compute these evaluation metrics for our regression models. Here's an example code for computing mean squared error for a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, 5, 2, 7, 8]\n",
    "y_pred = [2, 4, 3, 8, 6]\n",
    "print(\"Mean squared error:\", mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to these metrics, we may also need to compute other regression metrics such as mean absolute percentage error or root mean squared logarithmic error. We can use Scikit-learn to compute these metrics as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Model Parameters\n",
    "\n",
    "In supervised machine learning, the goal is to learn a function that maps input features to output labels, based on a set of training examples. This function is typically represented by a model, which has a set of parameters that can be learned from the training data. The model parameters define the shape and behavior of the model, and can be optimized to minimize the error between the predicted output and the actual output.\n",
    "\n",
    "For example, in linear regression, the model is represented by a linear equation that maps input features to a continuous output value. The model parameters are the coefficients of the equation, which can be learned from the training data using an optimization algorithm such as gradient descent.\n",
    "\n",
    "$$ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n $$\n",
    "\n",
    "where $y$ is the predicted output, $x_1, x_2, ..., x_n$ are the input features, $\\theta_0, \\theta_1, \\theta_2, ..., \\theta_n$ are the model parameters, and $n$ is the number of input features.\n",
    "\n",
    "In logistic regression, the model is represented by a sigmoid function that maps input features to a binary output value. The model parameters are the coefficients of the sigmoid function, which can be learned from the training data using an optimization algorithm such as gradient descent.\n",
    "\n",
    "$$ y = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n)}} $$\n",
    "\n",
    "where $y$ is the predicted output, $x_1, x_2, ..., x_n$ are the input features, $\\theta_0, \\theta_1, \\theta_2, ..., \\theta_n$ are the model parameters, and $n$ is the number of input features.\n",
    "\n",
    "The choice of model and model parameters depends on the specific problem and data at hand. In general, simpler models with fewer parameters are preferred over complex models with many parameters, to avoid overfitting and improve generalization performance. However, in some cases, more complex models may be necessary to capture the underlying complexity of the data.\n",
    "\n",
    "In practice, the model parameters are typically initialized with random values, and then optimized using an optimization algorithm such as gradient descent. The optimization algorithm iteratively updates the model parameters in the direction of the negative gradient of the loss function, which measures the error between the predicted output and the actual output.\n",
    "\n",
    "Here's an example code for training a linear regression model using gradient descent in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, num_epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.random.rand(n, 1)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            y_pred = X.dot(self.theta)\n",
    "            loss = np.mean((y_pred - y) ** 2)\n",
    "            grad = 2/m * X.T.dot(y_pred - y)\n",
    "            self.theta -= self.lr * grad\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"Loss:\", loss)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.theta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a class for linear regression models, with methods for fitting the model to data and making predictions. The `fit` method initializes the model parameters with random values, and then iteratively"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Hypothesis Function\n",
    "\n",
    "The hypothesis function is a mathematical function that maps input features to output predictions. In supervised learning, the goal is to learn the hypothesis function that best maps the input features to the output labels. The hypothesis function is typically represented by a model, which has a set of parameters that can be learned from data. The parameters are initialized with random values, and the goal is to find the optimal values that minimize the error between the predicted output and the actual output.\n",
    "\n",
    "The choice of hypothesis function depends on the specific problem and data at hand. In general, linear models are a popular choice for regression problems, while logistic models are a popular choice for classification problems. However, there are many other types of hypothesis functions that can be used, such as polynomial models, decision trees, neural networks, and support vector machines.\n",
    "\n",
    "For example, in linear regression, the hypothesis function is a linear equation that maps input features to a continuous output value:\n",
    "\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n $$\n",
    "\n",
    "where $x_1, x_2, ..., x_n$ are the input features, $\\theta_0, \\theta_1, \\theta_2, ..., \\theta_n$ are the model parameters, and $h_\\theta(x)$ is the predicted output.\n",
    "\n",
    "In logistic regression, the hypothesis function is a sigmoid function that maps input features to a binary output value:\n",
    "\n",
    "$$ h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}} $$\n",
    "\n",
    "where $x$ is the input features, $\\theta$ is the model parameters, $h_\\theta(x)$ is the predicted output, and $e$ is the base of the natural logarithm.\n",
    "\n",
    "In neural networks, the hypothesis function is a composition of multiple linear and nonlinear transformations, which can be represented by a series of layers. Each layer applies a linear transformation to the input features, followed by a nonlinear activation function such as a sigmoid, ReLU, or tanh.\n",
    "\n",
    "$$ h_\\theta(x) = f_3(f_2(f_1(xW_1 + b_1)W_2 + b_2)W_3 + b_3) $$\n",
    "\n",
    "where $x$ is the input features, $W_1, W_2, W_3$ are the weight matrices, $b_1, b_2, b_3$ are the bias vectors, and $f_1, f_2, f_3$ are the activation functions.\n",
    "\n",
    "The choice of activation function also depends on the specific problem and data at hand. In general, sigmoid and tanh functions are preferred for binary classification problems, while ReLU functions are preferred for regression and multiclass classification problems.\n",
    "\n",
    "Here's an example code for implementing a logistic regression model with a sigmoid hypothesis function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.theta = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.random.rand(n, 1)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            z = X.dot(self.theta)\n",
    "            y_pred = self.sigmoid(z)\n",
    "            loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "            grad = X.T.dot(y_pred - y) / m\n",
    "            self.theta -= self.lr * grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = X.dot(self.theta)\n",
    "        y_pred = self.sigmoid(z)\n",
    "        return y_pred > 0.5\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        acc = np.mean(y_pred == y)\n",
    "        return acc\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "model = LogisticRegression(lr=0.01, num_epochs=1000)\n",
    "model.fit(X, y)\n",
    "acc = model.score(X, y)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first define a `LogisticRegression` class that has methods for fitting the model parameters, predicting output labels, and computing the accuracy score. The `fit` method uses stochastic gradient descent to optimize the model parameters, while the `predict` method applies the sigmoid function to the input features and returns the predicted output labels. The `score` method compares the predicted output labels with the actual output labels and computes the accuracy score.\n",
    "\n",
    "Overall, the hypothesis function is a fundamental component of a machine learning model, and understanding how it works is essential for building effective models that can make accurate predictions on new data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
