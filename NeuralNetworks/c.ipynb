{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.5\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplatform\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPython version: \u001b[39m\u001b[39m{\u001b[39;00mplatform\u001b[39m.\u001b[39mpython_version()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[39massert\u001b[39;00m platform\u001b[39m.\u001b[39mpython_version_tuple() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m3\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m6\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "assert platform.python_version_tuple() >= (\"3\", \"6\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "A straight line function where activation is proportional to input ( which is the weighted sum from neuron ).\n",
    "\n",
    "**Function**\n",
    "\n",
    "$$ \n",
    "R(z,m) = \\begin{Bmatrix} z*m \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z,m):\n",
    "\treturn m*z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative**\n",
    "\n",
    "$$\n",
    "R'(z,m) = \\begin{Bmatrix} m \\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prime(z,m):\n",
    "\treturn m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Pros\n",
    ":class: tip\n",
    "- It gives a range of activations, so it is not binary activation.\n",
    "- We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that.\n",
    "```\n",
    "\n",
    "```{admonition} Cons\n",
    ":class: error\n",
    "- For this function, derivative is a constant. That means, the gradient has no relationship with X.\n",
    "- It is a constant gradient and the descent is going to be on constant gradient.\n",
    "- If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x) !\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU\n",
    "**Exponential Linear Unit** or its widely known name **ELU** is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.\n",
    "\n",
    "ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "\n",
    "**Function**\n",
    "\n",
    "$$\n",
    "R(z) = \\begin{Bmatrix} z & z > 0 \\\\\n",
    "α.( e^z – 1) & z <= 0 \\end{Bmatrix}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z,alpha):\n",
    "\treturn z if z >= 0 else alpha*(np.exp(z) -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU\n",
    "\n",
    "\n",
    "## ReLU\n",
    "\n",
    "\n",
    "## LeakyReLU\n",
    "\n",
    "\n",
    "## Softmax\n",
    "\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "\n",
    "## Tanh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
